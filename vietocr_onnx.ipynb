{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lhnguyen/.miniconda3/envs/kie-ner/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/lhnguyen/.miniconda3/envs/kie-ner/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/lhnguyen/.miniconda3/envs/kie-ner/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "from vietocr.tool.config import Cfg\n",
    "from vietocr.tool.predictor import Predictor\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class OCREncoder(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        \n",
    "        super(OCREncoder, self).__init__()\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, img):\n",
    "        src = self.model.cnn(img)\n",
    "        memory = self.model.transformer.forward_encoder(src)\n",
    "        return memory\n",
    "\n",
    "config = Cfg.load_config_from_name('vgg_transformer')\n",
    "\n",
    "\n",
    "config['weights']=\"./weights/vgg_transformer.pth\"\n",
    "config['device'] = 'cpu'\n",
    "\n",
    "trainer = Predictor(config)\n",
    "\n",
    "x = torch.randn(5, 3, 32, 160, requires_grad=True)\n",
    "\n",
    "model = OCREncoder(trainer.model)\n",
    "model.eval()\n",
    "rs = model(x)\n",
    "print(rs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_axes = {'input': {0: 'batch', 3: 'im_width'}, 'output': {0: 'feat_width', 1: 'batch'}}\n",
    "torch.onnx.export(model,               # model being run\n",
    "              x,                         # model input (or a tuple for multiple inputs)\n",
    "              \"transformer_encoder.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "              export_params=True,        # store the trained parameter weights inside the model file\n",
    "              opset_version=11,          # the ONNX version to export the model to\n",
    "              do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "              input_names = ['input'],   # the model's input names\n",
    "              output_names = ['output'], # the model's output names\n",
    "              dynamic_axes = dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 21:27:03.939213596 [E:onnxruntime:, inference_session.cc:1500 operator()] Exception during initialization: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/transpose.h:48 onnxruntime::TransposeBase::TransposeBase(const onnxruntime::OpKernelInfo&) v >= 0 && static_cast<uint64_t>(v) <= std::numeric_limits<size_t>::max() was false. \n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeException",
     "evalue": "[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/transpose.h:48 onnxruntime::TransposeBase::TransposeBase(const onnxruntime::OpKernelInfo&) v >= 0 && static_cast<uint64_t>(v) <= std::numeric_limits<size_t>::max() was false. \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnxruntime\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mort\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m sess \u001b[39m=\u001b[39m ort\u001b[39m.\u001b[39;49mInferenceSession(\u001b[39m'\u001b[39;49m\u001b[39m./transformer_encoder.onnx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.miniconda3/envs/kie-ner/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:347\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m disabled_optimizers \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    348\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/.miniconda3/envs/kie-ner/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:395\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    392\u001b[0m     disabled_optimizers \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    394\u001b[0m \u001b[39m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m sess\u001b[39m.\u001b[39;49minitialize_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    397\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess \u001b[39m=\u001b[39m sess\n\u001b[1;32m    398\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess\u001b[39m.\u001b[39msession_options\n",
      "\u001b[0;31mRuntimeException\u001b[0m: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/transpose.h:48 onnxruntime::TransposeBase::TransposeBase(const onnxruntime::OpKernelInfo&) v >= 0 && static_cast<uint64_t>(v) <= std::numeric_limits<size_t>::max() was false. \n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "sess = ort.InferenceSession('./transformer_encoder.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kie-ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
